## Introduction
PyTorch implementation for [Leveraging Smaller-Size Large Language Model and LoRA for Efficient Biomedical Named Entity Recognition]() . 

### News!

- [08/2024] Push Initial rep on Github and Release unofficial code, results, pretrained models. Citation, official code will be updated when the paper is accepted.


### Camel framework
```
NER has significant applicability in fields like information
retrieval, content organization, user experience enhancement, and more.
However, conventional NER practices grapple with limitations, such as
capturing broader context beyond individual sentences and handling
specialized domain knowledge required for tasks like Biomedical NER.
Despite the advances made by large language models (LLMs), and gen-
erative pre-trained transformer (GPT) models, challenges such as the
representation of negation and numerical aspects persist. Furthermore,
these modelsâ€™ lack the domain-specific knowledge needed for specialized
tasks like Biomedical NER, leading to high resource consumption and lim-
ited generalization. To address these issues, we propose a novel Biomedical
NER method using Smaller-Size Large Language Models (SLMs) to bal-
ance model training on common hardware settings without compromising
accuracy. We also introduce a Low-Rank Adaptation (LoRA) technique
to optimize SLMs without accessing full model weights, thereby reducing
computational load. The efficacy of our approach is validated through
extensive experiments on popular benchmarks like BC5, NCBI-disease,
BC2GM, and JNLPBA.
```



## Requirements and Datasets
- Same as [CompactBioedicalTransformer](https://github.com/nlpie-research/Compact-Biomedical-Transformers)


## Training and Evaluation

- Update later
 

### Experiment Results:
 - will be updated after paper is accepted


## Citation



