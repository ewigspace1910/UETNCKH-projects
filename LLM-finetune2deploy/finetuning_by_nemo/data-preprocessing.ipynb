{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12022532-b111-48f7-939b-8b3da863fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_jsonfile(file_path=\"toy_testset.jsonl\"):\n",
    "    all_samples = []\n",
    "    # Open and read the file line by line\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)  # Parse the JSON object from each line\n",
    "            # print(data)              # Do something with the JSON object\n",
    "            all_samples += [data]\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0430784-0e61-410c-bed4-888437cd082f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Context: The Panthers finished the regular season with a 15–1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49–15 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12–4 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20–18 in the AFC Championship Game. They joined the Patriots, Dallas Cowboys, and Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl. Question: What team did the Panthers defeat? Answer:',\n",
       " 'output': 'Arizona Cardinals',\n",
       " 'original_answers': ['Arizona Cardinals',\n",
       "  'the Arizona Cardinals',\n",
       "  'Arizona Cardinals']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_samples = read_jsonfile() \n",
    "model_samples[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d0c05f-f78a-44b1-8f17-9da8975c08c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read DREsS_Std.tsv\n",
      "Successfully read DREsS_New.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_all_tsv_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .tsv files in a folder and returns a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .tsv files.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of pandas DataFrames, each representing a .tsv file.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".tsv\"):  # Check if the file has a .tsv extension\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if not file_name in [\"DREsS_New.tsv\", \"DREsS_Std.tsv\"]: continue\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep='\\t')  # Read the TSV file\n",
    "                dataframes.append(df)  # Add it to the list\n",
    "                print(f\"Successfully read {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/workspace/datasets/DREsS\"  # Replace with the path to your folder\n",
    "tsv_data = read_all_tsv_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ad574a-cd8b-4a72-b742-89bf75ee3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                             prompt  \\\n",
      "0   1  More and more people use computers, but not ev...   \n",
      "1   2  More and more people use computers, but not ev...   \n",
      "2   3  More and more people use computers, but not ev...   \n",
      "3   4  More and more people use computers, but not ev...   \n",
      "4   5  More and more people use computers, but not ev...   \n",
      "\n",
      "                                               essay   content  organization  \\\n",
      "0  Dear local newspaper, I think effects computer...  3.333333      2.500000   \n",
      "1  Dear @CAPS1 @CAPS2, I believe that using compu...  3.333333      3.333333   \n",
      "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  2.500000      2.500000   \n",
      "3  Dear Local Newspaper, @CAPS1 I have found that...  4.166667      3.333333   \n",
      "4  Dear @LOCATION1, I know having computers has a...  3.333333      2.500000   \n",
      "\n",
      "   language      total  \n",
      "0  2.500000   8.333333  \n",
      "1  3.055556   9.722222  \n",
      "2  3.055556   8.055556  \n",
      "3  3.611111  11.111111  \n",
      "4  3.333333   9.166667  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_with_missing_columns(dataframes, required_columns):\n",
    "    \"\"\"\n",
    "    Concatenates a list of DataFrames, ensuring each has the required columns.\n",
    "    \n",
    "    Args:\n",
    "        dataframes (list): List of pandas DataFrames to concatenate.\n",
    "        required_columns (list): List of required column names.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: A single DataFrame with all required columns.\n",
    "    \"\"\"\n",
    "    updated_dataframes = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        # Add missing columns with NaN values\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        for col in missing_columns:\n",
    "            df[col] = None  # Add missing column with NaN values\n",
    "        \n",
    "        # Ensure the DataFrame has the correct column order\n",
    "        df = df[required_columns]\n",
    "        updated_dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into one\n",
    "    result = pd.concat(updated_dataframes, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "required_columns = [\"id\", \"prompt\", \"essay\", \"content\", \"organization\", \"language\", \"total\"]\n",
    "# Assuming `tsv_data` is your list of DataFrames\n",
    "final_dataframe = concatenate_with_missing_columns(tsv_data, required_columns)\n",
    "\n",
    "# Check the result\n",
    "print(final_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e8714f2-352a-4dda-9209-2bbb10a58364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>content</th>\n",
       "      <th>organization</th>\n",
       "      <th>language</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.055556</td>\n",
       "      <td>9.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.055556</td>\n",
       "      <td>8.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.611111</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>9.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>2275</td>\n",
       "      <td>If you could change one important aspect about...</td>\n",
       "      <td>What's the matter?\\n\\n\\n  Nowadays, many count...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>2276</td>\n",
       "      <td>If you could change one important aspect about...</td>\n",
       "      <td>I think there are lots of things to change a...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8784</th>\n",
       "      <td>2277</td>\n",
       "      <td>If you could change one important aspect about...</td>\n",
       "      <td>My country, Korea is a nice country. It is w...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8785</th>\n",
       "      <td>2278</td>\n",
       "      <td>If you could change one important aspect about...</td>\n",
       "      <td>Every country have its own culture and special...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>2279</td>\n",
       "      <td>If you could change one important aspect about...</td>\n",
       "      <td>Actually there are not many shortage of my cou...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8787 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0        1  More and more people use computers, but not ev...   \n",
       "1        2  More and more people use computers, but not ev...   \n",
       "2        3  More and more people use computers, but not ev...   \n",
       "3        4  More and more people use computers, but not ev...   \n",
       "4        5  More and more people use computers, but not ev...   \n",
       "...    ...                                                ...   \n",
       "8782  2275  If you could change one important aspect about...   \n",
       "8783  2276  If you could change one important aspect about...   \n",
       "8784  2277  If you could change one important aspect about...   \n",
       "8785  2278  If you could change one important aspect about...   \n",
       "8786  2279  If you could change one important aspect about...   \n",
       "\n",
       "                                                  essay   content  \\\n",
       "0     Dear local newspaper, I think effects computer...  3.333333   \n",
       "1     Dear @CAPS1 @CAPS2, I believe that using compu...  3.333333   \n",
       "2     Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  2.500000   \n",
       "3     Dear Local Newspaper, @CAPS1 I have found that...  4.166667   \n",
       "4     Dear @LOCATION1, I know having computers has a...  3.333333   \n",
       "...                                                 ...       ...   \n",
       "8782  What's the matter?\\n\\n\\n  Nowadays, many count...  2.000000   \n",
       "8783    I think there are lots of things to change a...  2.000000   \n",
       "8784    My country, Korea is a nice country. It is w...  4.000000   \n",
       "8785  Every country have its own culture and special...  3.500000   \n",
       "8786  Actually there are not many shortage of my cou...  3.000000   \n",
       "\n",
       "      organization  language      total  \n",
       "0         2.500000  2.500000   8.333333  \n",
       "1         3.333333  3.055556   9.722222  \n",
       "2         2.500000  3.055556   8.055556  \n",
       "3         3.333333  3.611111  11.111111  \n",
       "4         2.500000  3.333333   9.166667  \n",
       "...            ...       ...        ...  \n",
       "8782      2.000000  2.500000   6.500000  \n",
       "8783      2.000000  2.500000   6.500000  \n",
       "8784      3.500000  4.000000  11.500000  \n",
       "8785      3.000000  3.500000  10.000000  \n",
       "8786      3.000000  3.000000   9.000000  \n",
       "\n",
       "[8787 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b07b77a-f2ce-4884-8d87-2bcb8ca4836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type 1 | 2 | 3 | 4  =  0 0 0 8787\n",
      "8787\n",
      "For this essay, I will mark content_score as 4, organization_score as 3.5 and language_score as 4 \n",
      "For this essay, I will mark content_score as 3.5, organization_score as 3 and language_score as 3.5 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def round_score(x):\n",
    "    \"\"\"\n",
    "    Rounds the score according to the custom rule:\n",
    "      - If the fractional part is <= 0.25, round down.\n",
    "      - If the fractional part is > 0.25 and <= 0.75, round to the nearest 0.5.\n",
    "      - If the fractional part is > 0.75, round up.\n",
    "    \n",
    "    Examples:\n",
    "      3.25 -> 3\n",
    "      3.26 -> 3.5\n",
    "      3.75 -> 3.5\n",
    "      3.8  -> 4\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return x  # Return as is if it's not numeric\n",
    "    integer_part = int(score)\n",
    "    remainder = score - integer_part\n",
    "    if remainder <= 0.25:\n",
    "        return integer_part\n",
    "    elif remainder <= 0.75:\n",
    "        return integer_part + 0.5\n",
    "    else:\n",
    "        return integer_part + 1\n",
    "\n",
    "def generate_training_samples(dataframe):\n",
    "    \"\"\"\n",
    "    Generate training samples for an LLM for automated TOEFL essay scoring.\n",
    "    \n",
    "    Expected DataFrame columns:\n",
    "      - 'prompt': The question prompt.\n",
    "      - 'essay': The student's essay.\n",
    "      - 'content': Content score (range 0-5).\n",
    "      - 'organization': Organization score (range 0-5).\n",
    "      - 'language': Language score (range 0-5).\n",
    "      - 'total': (Optional) Composite total score.\n",
    "    \n",
    "    Rubrics:\n",
    "      - Organization: (range 0-5) The argument is very effectively structured and developed, \n",
    "        making it easy for the reader to follow the ideas and understand how the writer is building the argument. \n",
    "        Paragraphs use coherence devices effectively while focusing on a single main idea.\n",
    "      - Content: (range 0-5) The paragraph is well-developed and relevant to the argument, \n",
    "        supported with strong reasons and examples.\n",
    "      - Language: (range 0-5) The writing displays sophisticated control of a wide range of vocabulary and collocations. \n",
    "        The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\n",
    "    \n",
    "    The function applies a custom rounding to the score values as follows:\n",
    "      - If the fractional part is ≤ 0.25, round down.\n",
    "      - If the fractional part is > 0.25 and ≤ 0.75, round to the nearest 0.5.\n",
    "      - If the fractional part is > 0.75, round up.\n",
    "    \n",
    "    It then dynamically builds the training prompt based on the available score columns.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The merged DataFrame containing the necessary columns.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of training sample dictionaries with 'input', 'output', and 'original_answers' keys.\n",
    "    \"\"\"\n",
    "    \n",
    "    training_samples = []\n",
    "    \n",
    "    # Define the rubric text to include with every prompt.\n",
    "    # rubric_text = (\n",
    "    #     \"Rubrics:\\n\"\n",
    "    #     \"- Organization: (range 0-5) The argument is very effectively structured and developed, \"\n",
    "    #     \"making it easy for the reader to follow the ideas and understand how the writer is building the argument. \"\n",
    "    #     \"Paragraphs use coherence devices effectively while focusing on a single main idea.\\n\"\n",
    "    #     \"- Content: (range 0-5) The paragraph is well-developed and relevant to the argument, \"\n",
    "    #     \"supported with strong reasons and examples.\\n\"\n",
    "    #     \"- Language: (range 0-5) The writing displays sophisticated control of a wide range of vocabulary and collocations. \"\n",
    "    #     \"The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\\n\"\n",
    "    # )\n",
    "    t1=t2=t3=t4=0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Skip rows missing essential fields.\n",
    "        # Build the base context with the student essay and rubric definitions.\n",
    "        input_text = f\"Question: {row['prompt']}\\n\\n\"\n",
    "        input_text += f\"Essay: {row['essay']}\\n\\n\"\n",
    "        \n",
    "        # Determine which scores are available and build the task instruction accordingly.\n",
    "        if row.get('content') > -1 and row.get('organization') == -1 and row.get('language')==-1:\n",
    "            task_description = \"Please score the essay for the rubric dimension: Content (0-5).\"\n",
    "            input_text += f\"\\n{task_description}\"\n",
    "            output = f\"\"\"The content_score for this essay is {round_score(row['content'])}\"\"\"\n",
    "            t1+=1\n",
    "\n",
    "        elif row.get('content') == -1 and row.get('organization') > -1 and row.get('language')==-1:\n",
    "            task_description = \"Please score the essay for the rubric dimension: Organization (0-5).\"\n",
    "            input_text += f\"\\n{task_description}\"\n",
    "            output = f\"\"\"The organization_score for this essay is {round_score(row['organization'])}\"\"\"\n",
    "            t2+=1\n",
    "        elif row.get('content') == -1 and row.get('organization') == -1 and row.get('language')>-1:\n",
    "            task_description = \"Please score the essay for the rubric dimension: Language (0-5).\"\n",
    "            input_text += f\"\\n{task_description}\"\n",
    "            output = f\"\"\"The language_score for this essay is {round_score(row['language'])}\"\"\"\n",
    "            t3+=1\n",
    "        elif row.get('content') > -1 and row.get('organization') > -1 and row.get('language')>-1:\n",
    "            task_description = \"Please score the essay for the rubric dimensions: Content (0-5), Organization (0-5), and Language (0-5).\"\n",
    "            input_text += f\"\\n{task_description}\"\n",
    "            output = f\"\"\"For this essay, I will mark content_score as {round_score(row['content'])}, organization_score as {round_score(row['organization'])} and language_score as {round_score(row['language'])} \"\"\"\n",
    "            t4+=1\n",
    "        else:\n",
    "            # Skip rows that do not match any recognized scenario.\n",
    "            print(\"skip\", row)\n",
    "            continue\n",
    "\n",
    "        training_sample = {\n",
    "            'input': input_text.strip(),\n",
    "            'output': output,\n",
    "            'category': \"EssayScoring\",\n",
    "            \"taskname\": \"EssayScoring\"\n",
    "        }\n",
    "        training_samples.append(training_sample)\n",
    "    print(\"type 1 | 2 | 3 | 4  = \", t1, t2, t3, t4)\n",
    "    return training_samples\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'merged_dataframe' is your DataFrame with the merged TSV data and proper score columns.\n",
    "# training_data = generate_training_samples(merged_dataframe)\n",
    "# for sample in training_data[:3]:\n",
    "#     print(sample)\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'merged_dataframe' is the DataFrame with the required columns\n",
    "training_data = generate_training_samples(final_dataframe.fillna(-1))\n",
    "\n",
    "# Inspect the first few training samples\n",
    "print(len(training_data))\n",
    "for sample in training_data[-3:-1]:\n",
    "    print(sample['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec270fa-22cb-4414-b3c9-5565e1f61878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MMM', 'HMM', 'HHM', 'HHH', 'MMH', 'LLL', 'MML', 'MHH', 'LML', 'MLM', 'MHM', 'HMH', 'LMM', 'MLL', 'LLM', 'LMH', 'HLH', 'LHH', 'LHM'])\n",
      "Training data saved to /workspace/datasets/hey2/training.jsonl\n",
      "Validation data saved to /workspace/datasets/hey2/validation.jsonl\n",
      "Testing data saved to /workspace/datasets/hey2/testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random, json\n",
    "def extract_scores(text):\n",
    "    \"\"\"\n",
    "    Extracts content_score, language_score, and organization_score from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing the scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'content_score', 'language_score', and 'organization_score'.\n",
    "              Values are floats if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Define the regular expressions for each score\n",
    "    content_pattern = r\"content_score (?:for this essay )?\\w{2} (\\d+(\\.\\d+)?)\"\n",
    "    language_pattern = r\"language_score (?:for this essay )?\\w{2} (\\d+(\\.\\d+)?)\"\n",
    "    organization_pattern = r\"organization_score (?:for this essay )?\\w{2} (\\d+(\\.\\d+)?)\"\n",
    "    \n",
    "    # Search for matches in the text\n",
    "    content_match = re.search(content_pattern, text)\n",
    "    language_match = re.search(language_pattern, text)\n",
    "    organization_match = re.search(organization_pattern, text)\n",
    "    \n",
    "    # Extract values or set to None if not found\n",
    "    content_score = float(content_match.group(1)) if content_match else None\n",
    "    language_score = float(language_match.group(1)) if language_match else None\n",
    "    organization_score = float(organization_match.group(1)) if organization_match else None\n",
    "    def level_catgorize(point):\n",
    "        if point is None: return \"U\"\n",
    "        if point >= 4: return \"H\"\n",
    "        if point >= 2: return \"M\" \n",
    "        return \"L\"\n",
    "    total_score = level_catgorize(content_score) + level_catgorize(language_score) + level_catgorize(organization_score)\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"content_score\": content_score,\n",
    "        \"language_score\": language_score,\n",
    "        \"organization_score\": organization_score,\n",
    "        \"total_score\": total_score\n",
    "    }\n",
    "\n",
    "# Function to split dataset into train/validate/test\n",
    "def split_dataset(samples, test_size=100, valid_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the samples into training, validation, and test sets, ensuring equal label distribution in the test set.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): The list of sample dictionaries.\n",
    "        test_size (int): Number of samples in the test set.\n",
    "        valid_ratio (float): Proportion of the remaining data to allocate to validation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (training_set, validation_set, test_set)\n",
    "    \"\"\"\n",
    "    # Extract labels for each sample and group by label\n",
    "    labeled_samples = {}\n",
    "    for sample in samples:\n",
    "        # print(sample['output'])\n",
    "        label = extract_scores(sample['output'])['total_score']\n",
    "        if label is not None:\n",
    "            labeled_samples.setdefault(label, []).append(sample)\n",
    "    print(labeled_samples.keys())\n",
    "    # Create test set ensuring balanced label distribution\n",
    "    test_set = []\n",
    "    for label, group in labeled_samples.items():\n",
    "        # Proportional sampling for each label\n",
    "        num_samples = min(test_size // len(labeled_samples), len(group))\n",
    "        test_set.extend(random.sample(group, num_samples))\n",
    "    \n",
    "    # Remove test samples from the original dataset\n",
    "    remaining_samples = [sample for sample in samples if sample not in test_set]\n",
    "\n",
    "    # Split remaining samples into training and validation\n",
    "    valid_size = int(len(remaining_samples) * valid_ratio)\n",
    "    validation_set = random.sample(remaining_samples, valid_size)\n",
    "    training_set = [sample for sample in remaining_samples if sample not in validation_set]\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "\n",
    "# Save dataset to JSONL files\n",
    "def split_and_save_jsonl(\n",
    "    data,\n",
    "    test_size=100,\n",
    "    valid_ratio=0.2,\n",
    "    train_file=\"training.jsonl\",\n",
    "    valid_file=\"validation.jsonl\",\n",
    "    test_file=\"testing.jsonl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets, and saves them as JSONL files.\n",
    "\n",
    "    Args:\n",
    "        data (list): The list of samples to split, where each sample is a dictionary.\n",
    "        test_size (int): The number of samples to include in the test set.\n",
    "        valid_ratio (float): Proportion of the remaining data (after test split) to allocate to validation.\n",
    "        train_file (str): The filename for the training set.\n",
    "        valid_file (str): The filename for the validation set.\n",
    "        test_file (str): The filename for the testing set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Shuffle the data to ensure randomness.\n",
    "    train_data, valid_data, test_data = split_dataset(data, test_size=100, valid_ratio=0.2)\n",
    "\n",
    "    # Save the test data to a JSONL file.\n",
    "    with open(test_file, \"w\", encoding=\"utf-8\") as test_f:\n",
    "        for sample in test_data:\n",
    "            json.dump(sample, test_f)\n",
    "            test_f.write(\"\\n\")\n",
    "\n",
    "    # Save the validation data to a JSONL file.\n",
    "    with open(valid_file, \"w\", encoding=\"utf-8\") as valid_f:\n",
    "        for sample in valid_data:\n",
    "            json.dump(sample, valid_f)\n",
    "            valid_f.write(\"\\n\")\n",
    "\n",
    "    # Save the training data to a JSONL file.\n",
    "    with open(train_file, \"w\", encoding=\"utf-8\") as train_f:\n",
    "        for sample in train_data:\n",
    "            json.dump(sample, train_f)\n",
    "            train_f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Training data saved to {train_file}\")\n",
    "    print(f\"Validation data saved to {valid_file}\")\n",
    "    print(f\"Testing data saved to {test_file}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'training_data' is your list of generated samples:\n",
    "split_and_save_jsonl(\n",
    "    training_data,\n",
    "    test_size=100,\n",
    "    valid_ratio=0.1,\n",
    "    train_file=\"/workspace/datasets/hey2/training.jsonl\",\n",
    "    valid_file=\"/workspace/datasets/hey2/validation.jsonl\",\n",
    "    test_file=\"/workspace/datasets/hey2/testing.jsonl\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278c1f8-6304-4971-8b63-1d76ef2b7e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba199ee9-65ee-431d-9498-2ea8b9ce9287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99988ff7-0294-4db8-b3c6-1375802922b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
