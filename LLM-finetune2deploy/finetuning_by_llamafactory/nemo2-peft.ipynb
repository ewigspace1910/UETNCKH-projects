{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Optimizing Foundation Models with Parameter-Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "This notebook aims to demonstrate how to adapt or customize foundation models to improve performance on specific tasks using NeMo 2.0.\n",
    "\n",
    "This optimization process is known as fine-tuning, which involves adjusting the weights of a pre-trained foundation model with custom data.\n",
    "\n",
    "Considering that foundation models can be significantly large, a variant of fine-tuning has gained traction recently known as PEFT. PEFT encompasses several methods, including P-Tuning, LoRA, Adapters, IA3, etc. NeMo 2.0 currently supports [Low-Rank Adaptation (LoRA)](https://arxiv.org/pdf/2106.09685) method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1. Import the Hugging Face Checkpoint\n",
    "We use the `llm.import_ckpt` API to download the specified model using the \"hf://<huggingface_model_id>\" URL format. It will then convert the model into NeMo 2.0 format. For all model supported in NeMo 2.0, refer to [Large Language Models](https://docs.nvidia.com/nemo-framework/user-guide/24.09/llms/index.html#large-language-models) section of NeMo Framework User Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-04-10 10:34:02 nemo_logging:405] /opt/megatron-lm/megatron/core/transformer/cuda_graphs.py:741: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "      assert (\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1744281…</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1744281…\u001b[0m\u001b[92m ─\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1744281251/nemo.collections.llm.api.import_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10:34:11] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.import_ckpt for experiment </span>                     <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[10:34:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.import_ckpt for experiment \u001b[0m                     \u001b]8;id=174283;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=554323;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.import_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1744281251/nemo.collections.llm.api.import_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.import_ckpt-xxpgdp94x93v2c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.import_ckpt_1744281251 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.import_ckpt_1744281251 to finish\u001b[0m\u001b[92m ─────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt_1744281251</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt_1744281251\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.import_ckpt-xxpgdp94x93v2c\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1744281251/nemo.collections.llm.api.import_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.import_ckpt-xxpgdp94x93v2c\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1744281251/nemo.collections.llm.api.import_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.import_ckpt-xxpgdp94x93v2c to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mport_ckpt/0 Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.94it/s]\n",
      "mport_ckpt/0 [INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: False\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:35 nemo_logging:393] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n",
      "mport_ckpt/0 [INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "mport_ckpt/0 [INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "mport_ckpt/0 [NeMo W 2025-04-10 10:34:35 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "mport_ckpt/0 [INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 distributed_backend=gloo\n",
      "mport_ckpt/0 All distributed processes registered. Starting with 1 processes\n",
      "mport_ckpt/0 ----------------------------------------------------------------------------------------------------\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 [NeMo W 2025-04-10 10:34:35 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1090: `trainer.init_module` cannot fully support proper instantiation of your model with the `MegatronStrategy` strategy. Please instantiate your model inside the`LightningModule.configure_model` hook instead\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-04-10 10:34:38 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:38 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:34:42 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 0 : Start time: 1744281278.622s : Save duration: 4.005s\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:36:16 nemo_logging:393] Successfully saved checkpoint from iteration       0 to /root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B\n",
      "mport_ckpt/0 [NeMo I 2025-04-10 10:36:16 nemo_logging:393] Async finalization time took 94.290 s\n",
      "mport_ckpt/0 Converted Llama model to Nemo, model saved to /root/.cache/nemo/models/meta-llama/Meta-Llama-3-8B in torch.bfloat16.\n",
      "mport_ckpt/0 \u001b[32m $\u001b[0m\u001b[32mNEMO_MODELS_CACHE\u001b[0m\u001b[32m=\u001b[0m\u001b[32m/root/.cache/nemo/\u001b[0m\u001b[32mmodels\u001b[0m\u001b[32m \u001b[0m\n",
      "mport_ckpt/0 \u001b[32m✓ Checkpoint imported to \u001b[0m\u001b[32m/root/.cache/nemo/models/meta-llama/\u001b[0m\u001b[32mMeta-Llama-3-8B\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.import_ckpt-xxpgdp94x93v2c finished: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt_1744281251\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1744281251\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.import_ckpt_1744281251</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.import_ckpt_1744281251 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.import_ckpt_1744281251 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1744281251\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1744281251\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1744281251\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "from nemo.collections.llm.peft.lora import LoRA\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "\n",
    "\n",
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "# example usage:\n",
    "# llm.import_ckpt(model=llm.llama3_8b.model(), source=\"hf://meta-llama/Meta-Llama-3-8B\")\n",
    "#\n",
    "# We use run.Partial to configure this function\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3-8B\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "# configure your function\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "# define your executor\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# run your experiment\n",
    "run.run(import_ckpt, executor=local_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Prepare the Data\n",
    "\n",
    "We will be using SQuAD for this notebook. NeMo 2.0 already provides a `SquadDataModule`. Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squad() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(llm.SquadDataModule, seq_length=2048, micro_batch_size=1, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "import json\n",
    "import shutil\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nemo.collections.llm.gpt.data.core import get_dataset_root\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.utils import logging\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "    from nemo.collections.llm.gpt.data.packed_sequence import PackedSequenceSpecs\n",
    "class DollyDataModule(FineTuningDataModule, IOMixin):\n",
    "    \"\"\"A data module for fine-tuning on the Dolly dataset.\n",
    "\n",
    "    This class inherits from the `FineTuningDataModule` class and is specifically designed for fine-tuning models on the\n",
    "    \"databricks/databricks-dolly-15k\" dataset. It handles data download, preprocessing, splitting, and preparing the data\n",
    "    in a format suitable for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        force_redownload (bool, optional): Whether to force re-download the dataset even if it exists locally. Defaults to False.\n",
    "        delete_raw (bool, optional): Whether to delete the raw downloaded dataset after preprocessing. Defaults to True.\n",
    "        See FineTuningDataModule for the other args\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = False,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        packed_sequence_specs: Optional[\"PackedSequenceSpecs\"] = None,\n",
    "        dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "        print(get_dataset_root(\"dolly\"))\n",
    "        super().__init__(\n",
    "            dataset_root=get_dataset_root(\"dolly\"),\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            packed_sequence_specs=packed_sequence_specs,\n",
    "            dataset_kwargs=dataset_kwargs,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # if train file is specified, no need to do anything\n",
    "        if not self.train_path.exists() or self.force_redownload:\n",
    "            dset = self._download_data()\n",
    "            self._preprocess_and_split_data(dset)\n",
    "        super().prepare_data()\n",
    "\n",
    "    def _download_data(self):\n",
    "        logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "        return load_dataset(\n",
    "            \"databricks/databricks-dolly-15k\",\n",
    "            cache_dir=str(self.dataset_root),\n",
    "            download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "        )\n",
    "\n",
    "    def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "        logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "        test_ratio = 1 - train_ratio - val_ratio\n",
    "        save_splits = {}\n",
    "        dataset = dset.get('train')\n",
    "        split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "        split_dataset2 = split_dataset['test'].train_test_split(\n",
    "            test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "        )\n",
    "        save_splits['training'] = split_dataset['train']\n",
    "        save_splits['validation'] = split_dataset2['train']\n",
    "        save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "        for split_name, dataset in save_splits.items():\n",
    "            output_file = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "            with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for example in dataset:\n",
    "                    context = example[\"context\"].strip()\n",
    "                    if context != \"\":\n",
    "                        # Randomize context and instruction order.\n",
    "                        context_first = np.random.randint(0, 2) == 0\n",
    "                        if context_first:\n",
    "                            instruction = example[\"instruction\"].strip()\n",
    "                            assert instruction != \"\"\n",
    "                            _input = f\"{context}\\n\\n{instruction}\"\n",
    "                            _output = example[\"response\"]\n",
    "                        else:\n",
    "                            instruction = example[\"instruction\"].strip()\n",
    "                            assert instruction != \"\"\n",
    "                            _input = f\"{instruction}\\n\\n{context}\"\n",
    "                            _output = example[\"response\"]\n",
    "                    else:\n",
    "                        _input = example[\"instruction\"]\n",
    "                        _output = example[\"response\"]\n",
    "\n",
    "                    f.write(json.dumps({\"input\": _input, \"output\": _output, \"category\": example[\"category\"]}) + \"\\n\")\n",
    "\n",
    "            logging.info(f\"{split_name} split saved to {output_file}\")\n",
    "\n",
    "        if self.delete_raw:\n",
    "            for p in self.dataset_root.iterdir():\n",
    "                if p.is_dir():\n",
    "                    shutil.rmtree(p)\n",
    "                elif '.jsonl' not in str(p.name):\n",
    "                    p.unlink()\n",
    "    \n",
    "def dolly() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(CustomizedDataModule, seq_length=2048, micro_batch_size=1, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/nemo/datasets/dolly\n",
      "[NeMo I 2025-04-10 12:09:33 nemo_logging:393] Downloading DollyDataModule...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████████████████████████████████████| 15011/15011 [00:00<00:00, 345325.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-04-10 12:09:34 nemo_logging:393] Preprocessing DollyDataModule to jsonl format and splitting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-04-10 12:09:35 nemo_logging:393] training split saved to /root/.cache/nemo/datasets/dolly/training.jsonl\n",
      "[NeMo I 2025-04-10 12:09:35 nemo_logging:393] validation split saved to /root/.cache/nemo/datasets/dolly/validation.jsonl\n",
      "[NeMo I 2025-04-10 12:09:35 nemo_logging:393] test split saved to /root/.cache/nemo/datasets/dolly/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "ds = DollyDataModule(force_redownload=True)\n",
    "ds.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Which is a species of fish? Poacher or Hunter\", \"output\": \"Poacher\", \"category\": \"classification\"}\n",
      "{\"input\": \"The genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\\n\\nExtract all the movies from this passage and the year they were released out. Write each movie as a separate sentence\", \"output\": \"A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.\", \"category\": \"information_extraction\"}\n",
      "{\"input\": \"Give me three spicy authentic Chinese food dishes\", \"output\": \"There are many spicy Chinese food out there, among those the following are the local favorite dishes.\\n1. Chongqing Spicy Chicken\\n2. Kung Pao Chicken\\n3. Fish-Flavored Shredded Pork\", \"category\": \"brainstorming\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 100 /root/.cache/nemo/datasets/dolly/training.jsonl > hehe.jsonl\n",
    "head -n 3 hehe.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "from nemo.utils import logging\n",
    "from typing import Optional, List, Dict, Any, TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from nemo.collections.common.tokenizers import TokenizerSpec\n",
    "    from nemo.collections.llm.gpt.data.packed_sequence import PackedSequenceSpecs\n",
    "\n",
    "class DollyDataModule(FineTuningDataModule, IOMixin):\n",
    "    \"\"\"\n",
    "    A custom data module that uses pre-processed JSONL files as the data source.\n",
    "    \n",
    "    The expected files (in JSONL format) are:\n",
    "      - training.jsonl\n",
    "      - testing.jsonl\n",
    "    Optionally, if a validation split is available, use validation.jsonl.\n",
    "    \n",
    "    These files should reside under the provided dataset_root directory.\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): The maximum sequence length.\n",
    "        tokenizer (Optional[TokenizerSpec]): An initialized tokenizer.\n",
    "        micro_batch_size (int): The size for micro-batches.\n",
    "        global_batch_size (int): The overall batch size.\n",
    "        rampup_batch_size (Optional[List[int]]): Ramp-up schedule for the batch size.\n",
    "        delete_raw (bool): Not used here since the dataset is pre-processed.\n",
    "        seed (int): Random seed.\n",
    "        memmap_workers (int): Number of memmap workers.\n",
    "        num_workers (int): Number of workers to use for data loading.\n",
    "        pin_memory (bool): Whether to pin memory.\n",
    "        persistent_workers (bool): Use persistent workers.\n",
    "        packed_sequence_specs (Optional[PackedSequenceSpecs]): Specifications for packed sequences.\n",
    "        dataset_root (Optional[Path]): Root directory containing the JSONL files. Defaults to \"./custom_dataset\".\n",
    "        dataset_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        delete_raw: bool = False,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        packed_sequence_specs: Optional[\"PackedSequenceSpecs\"] = None,\n",
    "        dataset_root: Optional[Path] = Path(\"/workspace/datasets/hey2\"),\n",
    "        dataset_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        if dataset_root is None:\n",
    "            dataset_root = Path(\"./custom_dataset\")\n",
    "        self.dataset_root = dataset_root\n",
    "        # Define file paths for training, testing, and optionally validation.\n",
    "        self.train_path = self.dataset_root / \"training.jsonl\"\n",
    "        self.test_path = self.dataset_root / \"testing.jsonl\"\n",
    "        # If a validation file exists, it will be used.\n",
    "        self.validation_path = self.dataset_root / \"validation.jsonl\"\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_root=dataset_root,\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            packed_sequence_specs=packed_sequence_specs,\n",
    "            dataset_kwargs=dataset_kwargs,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Check for the existence of the pre-processed JSONL files.\n",
    "        If necessary files are missing, a FileNotFoundError is raised.\n",
    "        \"\"\"\n",
    "        if not self.train_path.exists():\n",
    "            raise FileNotFoundError(f\"Training file not found at: {self.train_path}\")\n",
    "        if not self.test_path.exists():\n",
    "            raise FileNotFoundError(f\"Testing file not found at: {self.test_path}\")\n",
    "        logging.info(\"Custom dataset files found at %s\", self.dataset_root)\n",
    "        # Call super() to allow further processing (e.g., tokenization and splitting into shards)\n",
    "        super().prepare_data()\n",
    "\n",
    "    def _setup_datasets(self):\n",
    "        \"\"\"\n",
    "        Load the training, testing (and optionally validation) datasets from the JSONL files.\n",
    "        The Hugging Face 'load_dataset' function is used to create a DatasetDict.\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading custom JSONL datasets...\")\n",
    "        data_files = {\"train\": str(self.train_path), \"test\": str(self.test_path)}\n",
    "        # Optionally add the validation split if the file exists.\n",
    "        if self.validation_path.exists():\n",
    "            data_files[\"validation\"] = str(self.validation_path)\n",
    "        dataset = load_dataset(\"json\", data_files=data_files)\n",
    "        logging.info(\"Custom datasets loaded successfully.\")\n",
    "        return dataset\n",
    "\n",
    "    # If needed, you can override other methods (e.g., setup, train_dataloader) \n",
    "    # to further control data preparation and batching.\n",
    "\n",
    "\n",
    "def customDS() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(DollyDataModule, seq_length=2048, micro_batch_size=1, global_batch_size=8, num_workers=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn how to use your own data to create a custom `DataModule` for performing PEFT, refer to [NeMo 2.0 SFT notebook](./nemo2-sft.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Configure PEFT with NeMo 2.0 API and NeMo-Run\n",
    "\n",
    "The following Python script utilizes the NeMo 2.0 API to perform PEFT. In this script, we are configuring the following components for training. These components are similar between SFT and PEFT. SFT and PEFT both use `llm.finetune` API. To switch from SFT to PEFT, you just need to add `peft` with the LoRA adapter to the API parameter.\n",
    "\n",
    "### Configure the Trainer\n",
    "The NeMo 2.0 Trainer works similarly to the PyTorch Lightning trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=1,\n",
    "        max_steps=20,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"nemo2_peft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )\n",
    "\n",
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=0.0001,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(\n",
    "        nl.MegatronOptimizerModule,\n",
    "        config=opt_cfg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass in the LoRA Adapter\n",
    "We need to pass in the LoRA adapter to our fine-tuning API to perform LoRA fine-tuning. We can configure the adapter as follows. The target module we support includes: `linear_qkv`, `linear_proj`, `linear_fc1` and `linear_fc2`. In the final script, we used the default configurations for LoRA (`llm.peft.LoRA()`), which will use the full list with `dim=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lora() -> run.Config[nl.pytorch.callbacks.PEFT]:\n",
    "    return run.Config(LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Base Model\n",
    "We will perform PEFT on top of Llama-3-8b, so we create a `LlamaModel` to pass to the NeMo 2.0 finetune API.\n",
    "### Auto Resume\n",
    "In NeMo 2.0, we can directly pass in the Llama3-8b Hugging Face ID to start PEFT without manually converting it into the NeMo checkpoint, as required in NeMo 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llama3_8b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama3Config8B))\n",
    "\n",
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(nl.RestoreConfig,\n",
    "            path=\"nemo://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure the NeMo 2.0 finetune API\n",
    "Using all the components we created above, we can call the NeMo 2.0 finetune API. The python example usage is as below:\n",
    "```\n",
    "llm.finetune(\n",
    "    model=llama3_8b(),\n",
    "    data=squad(),\n",
    "    trainer=trainer(),\n",
    "    peft=lora(),\n",
    "    log=logger(),\n",
    "    optim=adam_with_cosine_annealing(),\n",
    "    resume=resume(),\n",
    ")\n",
    "```\n",
    "We configure the `llm.finetune` API as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama3_8b(),\n",
    "        trainer=trainer(),\n",
    "        data=customDS(),\n",
    "        log=logger(),\n",
    "        peft=lora(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Run PEFT with NeMo 2.0 API and NeMo-Run\n",
    "\n",
    "We use `LocalExecutor` for executing our configured finetune function. For more details on the NeMo-Run executor, refer to [Execute NeMo Run](https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md) of NeMo-Run Guides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DollyDataModule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m executor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mDollyDataModule\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     run\u001b[38;5;241m.\u001b[39mrun(configure_finetuning_recipe(), executor\u001b[38;5;241m=\u001b[39mlocal_executor_torchrun())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DollyDataModule'"
     ]
    }
   ],
   "source": [
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import DollyDataModule\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Generate Results from Trained PEFT Checkpoints \n",
    "\n",
    "We use the `llm.generate` API in NeMo 2.0 to generate results from the trained PEFT checkpoint. Find your last saved checkpoint from your experiment dir: `results/nemo2_peft/checkpoints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will load PEFT checkpoint from: results/nemo2_peft/checkpoints/nemo2_peft--reduced_train_loss=0.2265-epoch=0-consumed_samples=160.0-last\n"
     ]
    }
   ],
   "source": [
    "peft_ckpt_path=str(next((d for d in Path(\"./results/nemo2_peft/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load PEFT checkpoint from:\", peft_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQuAD test set contains over 10,000 samples. For a quick demonstration, we will use the first 100 lines as an example input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Which NFL team represented the AFC at Super Bowl 50? Answer:\", \"output\": \"Denver Broncos\", \"original_answers\": [\"Denver Broncos\", \"Denver Broncos\", \"Denver Broncos\"]}\n",
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Which NFL team represented the NFC at Super Bowl 50? Answer:\", \"output\": \"Carolina Panthers\", \"original_answers\": [\"Carolina Panthers\", \"Carolina Panthers\", \"Carolina Panthers\"]}\n",
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Where did Super Bowl 50 take place? Answer:\", \"output\": \"Santa Clara, California\", \"original_answers\": [\"Santa Clara, California\", \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 100 /root/.cache/nemo/datasets/squad/test.jsonl > toy_testset.jsonl\n",
    "head -n 3 /root/.cache/nemo/datasets/squad/test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass the string `toy_testset.jsonl` to the `input_dataset` parameter of `llm.generate`. To evaluate the entire test set, you can instead pass the SQuAD data module directly, using `input_dataset=squad()`. The input JSONL file should follow the format shown above, containing `input` and `output` fields (additional keys are optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(peft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=\"toy_testset.jsonl\",\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=20, top_k=1),\n",
    "        output_path=\"peft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the inference is complete, you will see results similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Which NFL team represented the AFC at Super Bowl 50? Answer:\", \"original_answers\": [\"Denver Broncos\", \"Denver Broncos\", \"Denver Broncos\"], \"label\": \"Denver Broncos\", \"prediction\": \" Denver Broncos<|end_of_text|>\"}\n",
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Which NFL team represented the NFC at Super Bowl 50? Answer:\", \"original_answers\": [\"Carolina Panthers\", \"Carolina Panthers\", \"Carolina Panthers\"], \"label\": \"Carolina Panthers\", \"prediction\": \" Carolina Panthers<|end_of_text|>\"}\n",
      "{\"input\": \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: Where did Super Bowl 50 take place? Answer:\", \"original_answers\": [\"Santa Clara, California\", \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], \"label\": \"Santa Clara, California\", \"prediction\": \" Levi's Stadium<|end_of_text|><|begin_of_text|>://\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 3 peft_prediction.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Calculate Evaluation Metrics\n",
    "\n",
    "We can evaluate the model's predictions by calculating the Exact Match (EM) and F1 scores.\n",
    "- Exact Match is a binary measure (0 or 1) checking if the model outputs match one of the\n",
    "ground truth answer exactly.\n",
    "- F1 score is the harmonic mean of precision and recall for the answer words.\n",
    "\n",
    "Below is a script that computes these metrics. The sample scores can be improved by training the model further and performing hyperparameter tuning. In this notebook, we only train for 20 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match 0.000\tf1 29.133\trougeL 34.474\ttotal 100.000\n"
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/scripts/metric_calculation/peft_metric_calc.py --pred_file peft_prediction.jsonl --label_field \"original_answers\" --pred_field \"prediction\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
