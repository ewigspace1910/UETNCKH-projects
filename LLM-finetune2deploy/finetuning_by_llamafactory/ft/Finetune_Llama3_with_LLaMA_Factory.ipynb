{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"gra\",                         # use alpaca and identity datasets\n",
        "  template=\"llama4\",                                         # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                                    # use LoRA adapters to save memory\n",
        "  lora_rank= 8,\n",
        "  lora_target=\"all\",                                         # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"output/saves/llama4_lora\",                                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,                             # the micro batch size\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  logging_steps=10,                                           # log every 5 steps\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  save_steps=500,                                           # save checkpoint every 1000 steps\n",
        "  learning_rate=1e-4,                                        # the learning rate\n",
        "  num_train_epochs=3.0,                                      # the epochs of training\n",
        "  max_samples=1000,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                                                 # use float16 mixed precision training\n",
        "  report_to=\"none\",                                          # disable wandb logging\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama4.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO|2025-05-21 15:01:31] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file tokenizer.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file special_tokens_map.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file tokenizer_config.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,016 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-05-21 15:01:32,589 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-05-21 15:01:32,866 >> loading configuration file config.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-05-21 15:01:32,870 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.1\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,959 >> loading file tokenizer.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,959 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,959 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,959 >> loading file special_tokens_map.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,959 >> loading file tokenizer_config.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-05-21 15:01:32,960 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-05-21 15:01:33,520 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-05-21 15:01:33] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[WARNING|2025-05-21 15:01:33] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|2025-05-21 15:01:33] llamafactory.data.loader:143 >> Loading dataset /home/ducanh/nvidia-llm-pipeline/llamafactory/data/lra.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 1432, 2675, 527, 459, 358, 85128, 12365, 56614, 389, 27917, 950, 12027, 627, 22818, 264, 3488, 323, 220, 279, 10485, 2265, 11, 499, 15806, 279, 5575, 596, 11503, 323, 3041, 1124, 264, 5573, 627, 1547, 74029, 791, 27917, 950, 12027, 10485, 2265, 5764, 512, 14196, 4077, 66814, 2265, 25, 27917, 950, 1838, 70879, 320, 6102, 18607, 25, 220, 24, 340, 10793, 4078, 25, 8521, 198, 23340, 2996, 512, 220, 482, 18607, 25, 220, 24, 198, 262, 7817, 25, 482, 8797, 25152, 323, 24473, 1005, 527, 30576, 2949, 279, 7036, 315, 279, 3465, 627, 12, 362, 7029, 2134, 315, 36018, 374, 1511, 30357, 323, 36001, 449, 1633, 5933, 323, 27877, 2585, 315, 78686, 4519, 627, 58124, 6103, 304, 43529, 323, 3492, 18488, 527, 9193, 9024, 323, 617, 17832, 5536, 389, 10758, 627, 220, 482, 18607, 25, 220, 23, 198, 262, 7817, 25, 482, 362, 7029, 5211, 374, 20236, 4501, 323, 5882, 6623, 1511, 311, 20599, 24473, 50800, 2949, 279, 7036, 315, 279, 3465, 627, 12, 2684, 374, 1940, 321, 1285, 1005, 315, 41296, 323, 5255, 41760, 13795, 3673, 994, 8475, 11, 8994, 28961, 42025, 27121, 304, 3492, 5873, 323, 1400, 2588, 627, 12, 20522, 300, 4001, 6103, 304, 43529, 323, 5255, 3492, 18488, 1253, 12446, 11, 719, 617, 17832, 5536, 389, 10758, 627, 220, 482, 18607, 25, 220, 22, 198, 262, 7817, 25, 482, 578, 5211, 374, 14343, 311, 2187, 1063, 25152, 323, 16437, 627, 12, 2684, 374, 1063, 5845, 311, 1005, 2753, 4279, 323, 5255, 41760, 13795, 3673, 627, 482, 2127, 17985, 315, 1742, 323, 1400, 2588, 374, 30576, 11, 3582, 304, 76571, 27121, 12446, 627, 12, 2684, 527, 1193, 264, 2478, 6103, 304, 43529, 323, 5255, 3492, 18488, 11, 323, 814, 656, 539, 35453, 533, 505, 8244, 32373, 627, 220, 482, 18607, 25, 220, 21, 198, 262, 7817, 25, 482, 578, 5211, 374, 8965, 26613, 323, 8475, 369, 279, 3465, 627, 12, 578, 7438, 374, 8965, 2867, 304, 34781, 315, 264, 4856, 22486, 2134, 477, 264, 6996, 315, 16437, 304, 3492, 5873, 627, 12, 1442, 279, 7061, 374, 264, 5326, 2442, 4506, 11, 1070, 690, 387, 264, 22622, 2134, 315, 36018, 1511, 719, 5190, 12628, 315, 304, 33829, 477, 304, 76571, 2826, 627, 12, 2684, 527, 1063, 6103, 304, 43529, 323, 5255, 3492, 18488, 11, 719, 1521, 656, 539, 3242, 15686, 10758, 627, 220, 482, 18607, 25, 220, 20, 198, 262, 7817, 25, 482, 578, 5211, 374, 7347, 719, 21877, 750, 26613, 369, 279, 3465, 627, 12, 9170, 36018, 1253, 387, 1511, 30357, 719, 279, 2134, 1587, 539, 11810, 1790, 23851, 304, 7645, 627, 12, 2684, 1253, 387, 21420, 51055, 288, 304, 279, 7333, 2826, 315, 3492, 5873, 11, 323, 264, 6996, 315, 25152, 374, 10186, 304, 21420, 15858, 7174, 323, 5255, 86066, 627, 12, 40356, 304, 43529, 323, 5255, 3492, 18488, 1253, 387, 43426, 323, 1253, 5353, 1063, 17250, 369, 279, 6742, 627, 220, 482, 18607, 25, 220, 19, 198, 262, 7817, 25, 482, 578, 5211, 374, 7347, 323, 46579, 369, 477, 46305, 311, 279, 3465, 13, 99272, 374, 6913, 323, 1253, 387, 1511, 27427, 275, 3210, 627, 12, 2684, 1253, 387, 33781, 1005, 315, 78686, 27855, 320, 68, 1326, 13, 16420, 4147, 32847, 11, 15150, 292, 4221, 323, 5255, 4221, 505, 279, 1988, 3769, 4390, 12, 763, 29228, 3492, 5873, 323, 5255, 6103, 304, 3492, 18488, 323, 5255, 304, 43529, 1253, 3242, 15686, 7438, 627, 220, 482, 18607, 25, 220, 18, 198, 262, 7817, 25, 482, 578, 5211, 374, 46579, 320, 8370, 1253, 387, 4245, 311, 279, 2077, 1694, 12207, 1234, 4222, 4390, 12, 37207, 927, 36814, 768, 389, 1988, 3769, 477, 16420, 4147, 4221, 627, 12, 7935, 315, 3492, 5873, 323, 5255, 43529, 374, 1633, 7347, 11, 323, 6103, 4255, 316, 3357, 13, 4314, 6103, 1253, 35906, 3242, 15686, 7438, 627, 220, 482, 18607, 25, 220, 17, 198, 262, 7817, 25, 482, 578, 5211, 374, 9193, 7347, 449, 2478, 5952, 82585, 9246, 11, 10980, 505, 16420, 4147, 32847, 627, 12, 2684, 374, 912, 10186, 2585, 315, 3492, 18488, 323, 5255, 43529, 627, 220, 482, 18607, 25, 220, 16, 198, 262, 7817, 25, 482, 81567, 315, 220, 508, 4339, 477, 17162, 527, 22359, 520, 17366, 220, 16, 13, 720, 12, 2360, 5211, 374, 10186, 11, 3734, 369, 264, 2478, 25181, 4339, 627, 220, 482, 18607, 25, 220, 15, 198, 262, 7817, 25, 482, 12540, 1193, 387, 1511, 1405, 264, 9322, 1550, 539, 9604, 477, 4879, 279, 3488, 304, 904, 1648, 11, 1511, 264, 4221, 1023, 1109, 6498, 6957, 11, 477, 1405, 1070, 374, 11311, 430, 264, 9322, 753, 4320, 706, 1027, 12756, 16420, 4147, 382, 3134, 46008, 1432, 14196, 19884, 83445, 279, 2768, 4320, 1161, 8, 369, 358, 2818, 10155, 12365, 1296, 369, 279, 10485, 2265, 330, 65325, 38344, 278, 16842, 323, 51275, 3343, 19974, 311, 387, 26126, 25, 358, 2818, 10155, 12365, 1296, 961, 25, 3744, 220, 16, 198, 27504, 25, 25335, 198, 14924, 25, 5519, 499, 1093, 5403, 6603, 5380, 12, 3923, 13124, 315, 6603, 656, 499, 10932, 311, 1373, 5380, 12, 4438, 3629, 656, 499, 1373, 6603, 5380, 11529, 525, 701, 5403, 26870, 5614, 927, 892, 30, 720, 48332, 25, 7566, 11, 358, 8659, 4774, 5403, 6603, 13, 1102, 596, 264, 2294, 1648, 311, 82610, 323, 12731, 8903, 369, 264, 2766, 13, 73773, 279, 4595, 315, 6603, 11, 358, 2846, 5115, 78813, 11, 2216, 13, 358, 15763, 264, 1664, 67383, 54461, 311, 2567, 757, 389, 279, 6964, 315, 856, 10954, 11, 719, 358, 2846, 1101, 15107, 311, 2536, 74531, 11, 8104, 6160, 67245, 11, 1606, 358, 1505, 433, 27387, 311, 4048, 922, 279, 6439, 315, 32549, 12678, 13, 1628, 23781, 11, 358, 3358, 82845, 1139, 1063, 39074, 37731, 477, 18884, 11, 11911, 389, 856, 20247, 26, 264, 2766, 315, 89884, 2191, 2646, 13194, 5606, 11, 1314, 30, 358, 1456, 311, 1373, 1475, 1938, 11, 1524, 422, 433, 596, 1120, 369, 4376, 459, 6596, 1603, 4950, 13, 18056, 11, 79813, 11, 449, 990, 323, 1023, 42356, 11, 7170, 433, 10548, 709, 1694, 810, 1093, 264, 2478, 3115, 264, 2046, 11, 902, 374, 264, 21648, 13, 3092, 5403, 26870, 617, 5614, 5115, 264, 2766, 927, 279, 1667, 13, 3277, 358, 574, 14992, 11, 358, 3567, 21020, 4205, 323, 4395, 13, 4800, 11, 358, 2846, 1790, 810, 44010, 323, 8541, 311, 29059, 20227, 7119, 6603, 430, 3085, 1063, 3460, 315, 20207, 41959, 477, 264, 7878, 13356, 389, 279, 1917, 13, 128009, 128006, 78191, 128007, 271, 14023, 771, 16761, 2077, 32216, 264, 14343, 2134, 315, 36018, 449, 1063, 25152, 323, 16437, 13, 2405, 27663, 1093, 330, 359, 19703, 323, 12731, 8903, 1359, 330, 762, 68549, 1359, 330, 263, 279, 6964, 315, 856, 10954, 1359, 330, 258, 27256, 2335, 12678, 1359, 330, 9783, 588, 1139, 1359, 323, 330, 36490, 20227, 7119, 1, 13519, 459, 4879, 311, 1005, 2753, 4279, 323, 41760, 13795, 36018, 13, 2684, 596, 1101, 459, 17985, 315, 1742, 13, 4452, 11, 279, 304, 76571, 2826, 315, 279, 17571, 364, 64, 2766, 315, 89884, 2191, 2646, 13194, 5606, 11, 1314, 20837, 374, 264, 2766, 7669, 1697, 323, 2288, 42887, 11, 433, 374, 264, 4689, 5224, 430, 374, 17037, 1511, 11, 719, 433, 374, 539, 41760, 13795, 13, 362, 2478, 6103, 304, 3492, 5873, 527, 3118, 11, 323, 1524, 3582, 814, 1541, 956, 35453, 533, 279, 8244, 32373, 11, 433, 29034, 279, 2077, 311, 11322, 5190, 7200, 524, 27963, 1363, 55915, 11, 29937, 38344, 278, 16842, 612, 51275, 369, 420, 4320, 374, 551, 220, 22, 13, 15, 220, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "\n",
            "You are an Ielts speaking reviewer on Lexical Resource.\n",
            "Given a question and  the rubric, you evaluate the student's answers and give them a score.\n",
            "======================\n",
            "The Lexical Resource rubric includes:\n",
            "```\n",
            "Rubric: Lexical Resouce (Max Score: 9)\n",
            "Score Type: fixed\n",
            "Breakdown:\n",
            "  - Score: 9\n",
            "    Description: - Full flexibility and precise use are evident within the scope of the task.\n",
            "- A wide range of vocabulary is used accurately and appropriately with very natural and sophisticated control of lexical features.\n",
            "Minor errors in spelling and word formation are extremely rare and have minimal impact on communication.\n",
            "  - Score: 8\n",
            "    Description: - A wide resource is fluently and flexibly used to convey precise meanings within the scope of the task.\n",
            "- There is skilful use of uncommon and/or idiomatic items when appropriate, despite occasional inaccuracies in word choice and collocation.\n",
            "- Occasional errors in spelling and/or word formation may occur, but have minimal impact on communication.\n",
            "  - Score: 7\n",
            "    Description: - The resource is sufficient to allow some flexibility and precision.\n",
            "- There is some ability to use less common and/or idiomatic items.\n",
            " -An awareness of style and collocation is evident, though inappropriacies occur.\n",
            "- There are only a few errors in spelling and/or word formation, and they do not detract from overall clarity.\n",
            "  - Score: 6\n",
            "    Description: - The resource is generally adequate and appropriate for the task.\n",
            "- The meaning is generally clear in spite of a rather restricted range or a lack of precision in word choice.\n",
            "- If the writer is a risk-taker, there will be a wider range of vocabulary used but higher degrees of inaccuracy or inappropriacy.\n",
            "- There are some errors in spelling and/or word formation, but these do not impede communication.\n",
            "  - Score: 5\n",
            "    Description: - The resource is limited but minimally adequate for the task.\n",
            "- Simple vocabulary may be used accurately but the range does not permit much variation in expression.\n",
            "- There may be frequent lapses in the appropriacy of word choice, and a lack of flexibility is apparent in frequent simplifications and/or repetitions.\n",
            "- Errors in spelling and/or word formation may be noticeable and may cause some difficulty for the reader.\n",
            "  - Score: 4\n",
            "    Description: - The resource is limited and inadequate for or unrelated to the task. Vocabulary is basic and may be used repetitively.\n",
            "- There may be inappropriate use of lexical chunks (e.g. memorised phrases, formulaic language and/or language from the input material).\n",
            "- Inappropriate word choice and/or errors in word formation and/or in spelling may impede meaning.\n",
            "  - Score: 3\n",
            "    Description: - The resource is inadequate (which may be due to the response being significantly underlength).\n",
            "- Possible over-dependence on input material or memorised language.\n",
            "- Control of word choice and/or spelling is very limited, and errors predominate. These errors may severely impede meaning.\n",
            "  - Score: 2\n",
            "    Description: - The resource is extremely limited with few recognisable strings, apart from memorised phrases.\n",
            "- There is no apparent control of word formation and/or spelling.\n",
            "  - Score: 1\n",
            "    Description: - Responses of 20 words or fewer are rated at Band 1. \n",
            "- No resource is apparent, except for a few isolated words.\n",
            "  - Score: 0\n",
            "    Description: - Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate’s answer has been totally memorised.\n",
            "\n",
            "===============================================\n",
            "\n",
            "\n",
            "```\n",
            "\n",
            "Evaluate the following answer(s) for IELTS speaking test for the rubric \"Grammatical Range and Accuracy\". Items to be evaluated: IELTS speaking test part: Part 1\n",
            "Topic:Books\n",
            "Question:Do you like reading books?\n",
            "-What kinds of books do you prefer to read?\n",
            "-How often do you read books?\n",
            "-Have your reading habits changed over time? \n",
            "Answers: Yes, I definitely enjoy reading books. It's a great way to unwind and escape reality for a bit. Regarding the types of books, I'm quite eclectic, really. I appreciate a well-written thriller to keep me on the edge of my seat, but I'm also drawn to non-fiction, particularly biographies, because I find it fascinating to learn about the lives of influential figures. And occasionally, I'll delve into some sci-fi or fantasy, depending on my mood; a bit of escapism never hurt anyone, right? I try to read every day, even if it's just for half an hour before bed. Though, admittedly, with work and other commitments, sometimes it ends up being more like a few times a week, which is a shame. My reading habits have changed quite a bit over the years. When I was younger, I devoured anything and everything. Now, I'm much more selective and tend to gravitate towards books that offer some sort of intellectual stimulation or a fresh perspective on the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<think>The response demonstrates a sufficient range of vocabulary with some flexibility and precision. Phrases like \"unwind and escape reality,\" \"eclectic,\" \"on the edge of my seat,\" \"influential figures,\" \"delve into,\" and \"gravitate towards\" indicate an attempt to use less common and idiomatic vocabulary. There's also an awareness of style. However, the inappropriacy of the phrase 'a bit of escapism never hurt anyone, right?' is a bit conversational and too informal, it is a general statement that is commonly used, but it is not idiomatic. A few errors in word choice are present, and even though they don't detract the overall clarity, it prevents the response to achieve higher band</think>\n",
            "\n",
            "Therefore, Grammatical Range & Accuracy for this answer is : 7.0 <|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 14023, 771, 16761, 2077, 32216, 264, 14343, 2134, 315, 36018, 449, 1063, 25152, 323, 16437, 13, 2405, 27663, 1093, 330, 359, 19703, 323, 12731, 8903, 1359, 330, 762, 68549, 1359, 330, 263, 279, 6964, 315, 856, 10954, 1359, 330, 258, 27256, 2335, 12678, 1359, 330, 9783, 588, 1139, 1359, 323, 330, 36490, 20227, 7119, 1, 13519, 459, 4879, 311, 1005, 2753, 4279, 323, 41760, 13795, 36018, 13, 2684, 596, 1101, 459, 17985, 315, 1742, 13, 4452, 11, 279, 304, 76571, 2826, 315, 279, 17571, 364, 64, 2766, 315, 89884, 2191, 2646, 13194, 5606, 11, 1314, 20837, 374, 264, 2766, 7669, 1697, 323, 2288, 42887, 11, 433, 374, 264, 4689, 5224, 430, 374, 17037, 1511, 11, 719, 433, 374, 539, 41760, 13795, 13, 362, 2478, 6103, 304, 3492, 5873, 527, 3118, 11, 323, 1524, 3582, 814, 1541, 956, 35453, 533, 279, 8244, 32373, 11, 433, 29034, 279, 2077, 311, 11322, 5190, 7200, 524, 27963, 1363, 55915, 11, 29937, 38344, 278, 16842, 612, 51275, 369, 420, 4320, 374, 551, 220, 22, 13, 15, 220, 128009]\n",
            "labels:\n",
            "<think>The response demonstrates a sufficient range of vocabulary with some flexibility and precision. Phrases like \"unwind and escape reality,\" \"eclectic,\" \"on the edge of my seat,\" \"influential figures,\" \"delve into,\" and \"gravitate towards\" indicate an attempt to use less common and idiomatic vocabulary. There's also an awareness of style. However, the inappropriacy of the phrase 'a bit of escapism never hurt anyone, right?' is a bit conversational and too informal, it is a general statement that is commonly used, but it is not idiomatic. A few errors in word choice are present, and even though they don't detract the overall clarity, it prevents the response to achieve higher band</think>\n",
            "\n",
            "Therefore, Grammatical Range & Accuracy for this answer is : 7.0 <|eot_id|>\n",
            "[INFO|configuration_utils.py:698] 2025-05-21 15:01:33,953 >> loading configuration file config.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-05-21 15:01:33,954 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.1\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-05-21 15:01:33] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2025-05-21 15:01:33] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|quantization_config.py:506] 2025-05-21 15:01:34,247 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
            "    lib = get_native_library()\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
            "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
            "    return self._dlltype(name)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
            "\n",
            "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
            "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
            "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
            "\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100%|████████████████████| 5.70G/5.70G [00:56<00:00, 101MB/s]\n",
            "[INFO|modeling_utils.py:1149] 2025-05-21 15:02:31,076 >> loading weights file model.safetensors from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2239] 2025-05-21 15:02:31,081 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1135] 2025-05-21 15:02:31,084 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5170] 2025-05-21 15:02:38,733 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5178] 2025-05-21 15:02:38,733 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100%|██████████████████| 220/220 [00:00<00:00, 1.09MB/s]\n",
            "[INFO|configuration_utils.py:1090] 2025-05-21 15:02:38,842 >> loading configuration file generation_config.json from cache at /home/ducanh/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-05-21 15:02:38,842 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-05-21 15:02:39] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-05-21 15:02:39] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-05-21 15:02:39] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-05-21 15:02:39] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-05-21 15:02:39] llamafactory.model.model_utils.misc:143 >> Found linear modules: v_proj,up_proj,gate_proj,q_proj,k_proj,down_proj,o_proj\n",
            "[INFO|2025-05-21 15:02:40] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "Detected kernel version 4.19.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "[INFO|trainer.py:756] 2025-05-21 15:02:40,457 >> Using auto half precision backend\n",
            "[INFO|2025-05-21 15:02:40] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2409] 2025-05-21 15:02:40,843 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-05-21 15:02:40,843 >>   Num examples = 2,000\n",
            "[INFO|trainer.py:2411] 2025-05-21 15:02:40,843 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-05-21 15:02:40,843 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-05-21 15:02:40,844 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-05-21 15:02:40,844 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-05-21 15:02:40,844 >>   Total optimization steps = 750\n",
            "[INFO|trainer.py:2418] 2025-05-21 15:02:40,849 >>   Number of trainable parameters = 20,971,520\n",
            "  0%|                                                   | 0/750 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
            "    COMMAND_MAP[command]()\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 96, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/trainer.py\", line 2240, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/trainer.py\", line 3745, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\", line 103, in compute_loss\n",
            "    return super().compute_loss(model, inputs, *args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/trainer.py\", line 3810, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 818, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 806, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/peft/peft_model.py\", line 1757, in forward\n",
            "    return self.base_model(\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 193, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 47, in __call__\n",
            "    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n",
            "  File \"/home/ducanh/nvidia-llm-pipeline/llamafactory/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py\", line 97, in custom_gradient_checkpointing_func\n",
            "    return gradient_checkpointing_func(func, *args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/_compile.py\", line 51, in inner\n",
            "    return disable_fn(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 488, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 263, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n",
            "    hidden_states, self_attn_weights = self.self_attn(\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 242, in forward\n",
            "    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 494, in forward\n",
            "    result = self.base_layer(x, *args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n",
            "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n",
            "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1353, in dequantize_4bit\n",
            "    absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)\n",
            "  File \"/home/ducanh/data/miniconda3/envs/llamafac/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1044, in dequantize_blockwise\n",
            "    lib.cdequantize_blockwise_fp32(*args)\n",
            "AttributeError: 'NoneType' object has no attribute 'cdequantize_blockwise_fp32'\n",
            "  0%|                                                   | 0/750 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli train train_llama3.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "## Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                         # same to the one in training\n",
        "  finetuning_type=\"lora\",                                    # same to the one in training\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTESHaFvbNTr"
      },
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",                       # load the saved LoRA adapters\n",
        "  template=\"llama3\",                                        # same to the one in training\n",
        "  finetuning_type=\"lora\",                                   # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",                          # the path to save the merged model\n",
        "  export_size=2,                                            # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                                      # the device used in export, can be chosen from `cpu` and `auto`\n",
        "  # export_hub_model_id=\"your_id/your_model\",               # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llamafac",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
